1. 객체탐지
   1. 이미지내에서 사물을 탐지해내는 기술
2. 평가지표
   1. IOU
      1. 이미지 내에 있는 하나의 객체를 탐지할 때 사용하는 평가지표
      2. 실제 객체면적과 모델이 예측한 면적의 교차영역/전체영역
      3. 범위는 0~1.0 사이(일반적으로 0.5가 넘으면 맞게 예측했다고 판단)
   2. mAP(mean Average Precision)
      1. 한 이미지에서 여러 개의 (클래스)를 탐지할 때 사용되는 평가지표
      2. 클래스 별 평균 정밀도(AP)를 계산 후 클래스 전체에 대한 평균을 구함
      3. 범위는 0~ 1.0사이, IoU가 상승하면 mAP하락
      4. AP(Average orecision)
         1. 일반적으로 정밀도와 재현율은 반비례 관계
         2. 두 값을 모두 고려하여 모델의 성능을 판단하는 것이 더 합리적
         3. 재현율의 11개 지점애 대해 정밀도를 각각 구해서 이를 평균낸 것이 AP
3. 객체 탐지 알고리즘의 변천사
   1. Traditional Detection Methods
      1. 슬라이딩 윈도우(Sliding Window)
         1. 고정된 크기의 Window로 이미지의 좌상단부터 우하단으로 일일이 객체를 검출해 나가는 방식
      2. 문제
         1. 객체가 없는 영역도 무조건 Sliding해야 하며 이미지의 Scale을 조절해서 스캔하며 검출하는 방식이므로 수행시간이 긴 것에 비해 검출 성능↓
   2.  Two-stage detector
      1. 영역추정(Region Proposal)과 탐지(Detection) 두 단계를 따로 수행
      2. Sliding Window의 비효율성으로 인해 R-CNN 알고리즘에서는 ‘객체가 있을 법한 2000개의 영역’을 찾고 ‘그 영역에 대해서만 객체를 탐지’하는 두 단계를 제안
      3. R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN 등의 모델이 있음
      4. 문제점
         1. 객체들이 각기 다른 크기와 형태를 가지고 있다면 후보 영역을 찾는 정확도 ↓  
         2. 영역추정의 정확도를 향상시키기 위해 미리 이미지에서 객체 영역을 분할해 둠
   3. 선택적 검색(Selective Search)
      1. 처음에는 분할된 모든 부분들을 Bounding box로 만들어 리스트에 추가
      2. 색상, 무늬, 크기, 형태에 따라 유사도가 비슷한 부분들을 그룹핑(Bbox 개수 감소)
      3. 1, 2 단계 지속 반복
   4. One-stage detector
      1. Two-stage detector는 Selective search 방식으로 인해 과거 대비 높은 정확도로 객체 탐지가 가능했지만, 여전히 낮은 속도로 실시간 적용은 어려웠음
      2. One-stage detector는 영역추정과 객체탐지를 통합해 한 번에 수행
      3. 가장 큰 장점은 탐지 속도의 획기적인 향상으로 실시간 탐지가 가능
4. Yolo
   1. One-stage detector방식의 가장 잘 알려진 실시산 객체 탐지 알고리즘
   2. 2016년 version1부터 2023년 version8까지 오픈소스로 출시됨
   3. Yolo v1(GoogleNet 적용)은 Two-stage detector의 Faster R-CNN (vgg16 적용)보다 6배 빠른 속도로 논문에 기재됨
   4. v1
      1. 각각의 Bbox는 x, y, w, h 와 Confidence Score로 구성됨
      2. x, y는 Bbox의 중심점, w, h 는 너비, 높이
      3. Confidence Score는 Bbox가 객체를 포함한다는 예측 확신 정도의 지표
      4. Pr : Grid cell 내에 물체가 존재할 확률 (존재하면 1, 아니면 0)
      5. Confidence Score가 0.5 이하인 Bbox는 모두 삭제 (기준은 사용자 지정 가능)
      6. 값이 가장 높은 Bbox만 남기고 나머지는 삭제하여 한 객체당 하나의 Bbox만 남김
   5. v2
      1. Bbox의 개수를 늘리고 GoogelNet 대신 Darknet-19 모델을 사용하여 v1에 비해 mAP 향상
      2. Multi-Scaling기법을 사용하여 v1의 문제점인 작은 객체에 대한 인식률 향상
   6. v3
      1. Darknet-53 모델로 변경하고 내부 구조를 조정하여 FPS를 2배 이상 향상
      2. 특성맵의 크기를 조절하여 크기가 큰 객체의 검출 성능 향상
      3. 다수의 객체 예측시 softmax 대신 클래스 별 sigmoid를 활용하여 검출 (하나의 Bbox안에 복수의 객체가 존재하는 경우 softmax성능 ↓)
   7. v4
      1. CSPDarknet53, SPP, PAN, BoF, Bos 등의 기법을 통해 v3에 비해 mAP, FPS를 각각 10%, 12%씩 향상
   8. v6
      1. 논문 없이 깃 허브로 코드만 공유, Darknet 대신 Pytorch로 구현
      2. v4에 비해 낮은 용량, 빠른 속도(높은 FPS), 비슷한 성능(mAP)
      3. 검출되는 객체의 크기 별 전용 버전인 s, m, l, x로 버전 세분화
      4. Pascal VOC 데이터 대신 COCO 데이터 셋(20만개, 클래스 80개)으로 훈련
   9. v7
      1. COCO 데이터셋 기준 AP 56.8%로 7까지의 Yolo 버전 중 가장 성능이 높음
      2. 인간의 포즈를 추정할 수 있는 포즈추정 모델 포함(Yolo에서 첫 등장)
   10. v8
       1. 내부 구조를 변경하여 이전버전에 비해 평균적으로 mAP가 높음
       2. 객체 탐지, 인스턴스 세분화, 이미지 분류를 위한 통합 프레임워크로 구축