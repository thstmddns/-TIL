1. 앙상블

   1. 여러 개의 모델이 예측한 값을결합하여 정확한 최정 예측을 도출하는 기법 

   2. 앙상블을 사용하는 이유

      1. 단일 모델에 비해 높은 성능과 신뢰성을 얻을 수 있음
      2. 데이터의 양이 적은 것에 비해 충분한 학습 효과를 거둘 수 있음

   3. 보팅

      1. 여러 개의 다른 종류의 모델이 예측한 결과를 투표 혹은 평균을 통해 최종 선정

   4. 배깅

      1. 여러 개의 같은 종류의 모델이 예측한 결과를 투표 혹은 평균을 통해 최종 선정(데이터 샘플링을 다르게, 중첩 허용)
      2. 데이터 샘플링에서 중첩을 허용하지만 확률적으로 중첩될 학률이 동일하므로 허용가능하다.

   5. 부스팅

      1. 여러 개의 같은 종류의 모델이 순차적으로 학습-예측하며 오류를 개선하는 방식(결정 트리 모형의 베이스로 사용)

   6. 배깅 vs 부스팅

      1. |             | 배깅                                                         | 부스팅                                                       |
         | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
         | 특징        | 같은 종류의 모델이 투표를 통해 최종 예측 결과를 도출(데이터 샘플을 다르게 가져감) | 순차학습+예측(이전 모델의 오류를 고려)                       |
         | 목적        | 일반적으로 좋은 모델을 만들기 위해 과대적합 방지(편향된 학습을 방지) | 맞추기 어려운 문제를 풀기 위해 과소적합 방지(부족한 학습을 방지) |
         | 적합한 상황 | 데이터 값들의 편차가 클 경우                                 | 학습 정확도가 낮거나 오차가 클 경우                          |
         | 대표 모델   | Random forest                                                | Ada Boosting, Gradient Boosting, XG Boosting, LightGBM       |
         | 데이터 선택 | 무작위 선택                                                  | 무작위 선택(오류 데이터에 가중치 적용)                       |

   7. 장점

      1. 실제 값에 대한 추정한 오차 평균화, 분산 감소, 과적합 감소
      2. 부스팅 방식에 비해 빠른 수행 속도
      3. 모델 튜닝을 위한 시간이 많이 필요
      4. 큰 데이터 세트에도 잘 동작하지만, 트리의 개수가 많아질수록 시간이 오래걸림 

2. 랜덤포레스트

   1. 앙상블의 한 종류로 결정트리의 일종이다.
   2. 트리의 개수 : n_estimators
   3. 선택할 특징의 최대 수 : max_features
   4. 선택할 데이터 시드 수 : random_state

3. Ada Boosting

   1. Rf처럼 의사결정 트리기반의 모델 -> 각각의 트리들이 독립적으로 존재하지 않음
   2. 동작순서
      1. 첫번째 의사결정 트리를 생성 
      2. 잘못된 분류에는 높은 가중치를 부여하고 맞은 것에는 낮은 가중치를 부여
      3. 가중치를 부여한 상태에서 다시 분류시킴
         1. 진행한 분류들을 결합한다.

4. 경사하강법

   1. 경사가 완만해질수록 오차가 작아진다.

5. 그레디언트 부스팅

   1. 장단점
      1. 학습 속도가 느림(부스팅의 일반적인 단점 -> 순차적인 학습을 수행하므)
      2. 데이터 특성의 스케일을 조정할 필요가 없음(트리 기반 모델의 특성)

6. XG부스팅

   1. GBM의 단점 : 느림, 과대적합 문제
   2. GBM보다 빠름: Early Stopping 제공
   3. 과대 적합 방지를 위한 규제 포함
   4. 병렬로 빠른 학습이 가능

7. lightGBM

   1. XG Boosting에 비해 가벼워 속도가 빠른 모델
      1. Leaf-wise(수직방향, 비대칭)로 트리를 성장시킴(속도up)
      2. level-wise(수평방향, 깊이 down, 대칭)보다 오류가 더 적음(정확도 up)
   2. 장점
      1. 대량(1만개 이상)의 데이터를 병렬로 빠르게 학습가능(Low Memory, GPU 활용 가능)
         1. XG Boosting 대비 2~10배의 속도(동일 파라미터 설정 시)
         2. 예측 속도가 빠름(Leaf-wise 트리의 장점)
   3. 단점
      1. 소량의 데이터에서는 제대로 동작하지 않음(과대 적합 위험)