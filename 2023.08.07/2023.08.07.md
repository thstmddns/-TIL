1. 앙상블

   1. 여러 개의 모델이 예측한 값을결합하여 정확한 최정 예측을 도출하는 기법 

   2. 앙상블을 사용하는 이유

      1. 단일 모델에 비해 높은 성능과 신뢰성을 얻을 수 있음
      2. 데이터의 양이 적은 것에 비해 충분한 학습 효과를 거둘 수 있음

   3. 보팅

      1. 여러 개의 다른 종류의 모델이 예측한 결과를 투표 혹은 평균을 통해 최종 선정

   4. 배깅

      1. 여러 개의 같은 종류의 모델이 예측한 결과를 투표 혹은 평균을 통해 최종 선정(데이터 샘플링을 다르게, 중첩 허용)
      2. 데이터 샘플링에서 중첩을 허용하지만 확률적으로 중첩될 학률이 동일하므로 허용가능하다.

   5. 부스팅

      1. 여러 개의 같은 종류의 모델이 순차적으로 학습-예측하며 오류를 개선하는 방식(결정 트리 모형의 베이스로 사용)

   6. 배깅 vs 부스팅

      1. |             | 배깅                                                         | 부스팅                                                       |
         | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
         | 특징        | 같은 종류의 모델이 투표를 통해 최종 예측 결과를 도출(데이터 샘플을 다르게 가져감) | 순차학습+예측(이전 모델의 오류를 고려)                       |
         | 목적        | 일반적으로 좋은 모델을 만들기 위해 과대적합 방지(편향된 학습을 방지) | 맞추기 어려운 문제를 풀기 위해 과소적합 방지(부족한 학습을 방지) |
         | 적합한 상황 | 데이터 값들의 편차가 클 경우                                 | 학습 정확도가 낮거나 오차가 클 경우                          |
         | 대표 모델   | Random forest                                                | Ada Boosting, Gradient Boosting, XG Boosting, LightGBM       |
         | 데이터 선택 | 무작위 선택                                                  | 무작위 선택(오류 데이터에 가중치 적용)                       |

   7. 장점

      1. 실제 값에 대한 추정한 오차 평균화, 분산 감소, 과적합 감소
      2. 부스팅 방식에 비해 빠른 수행 속도
      3. 모델 튜닝을 위한 시간이 많이 필요
      4. 큰 데이터 세트에도 잘 동작하지만, 트리의 개수가 많아질수록 시간이 오래걸림 

2. 랜덤포레스트

   1. 앙상블의 한 종류로 결정트리의 일종이다.
   2. 트리의 개수 : n_estimators
   3. 선택할 특징의 최대 수 : max_features
   4. 선택할 데이터 시드 수 : random_state