1. 오차역전파
   1. 순파
      1. 입력데이터를 입력층에서부터 출력층에서부터 출력층까지 정방향으로 이동시키며 예측해나가는 과정
   2. 역전파
      1. 출력층부터 발생한 에러를 입력층 쪽으로 전파시키면서 최적의 결과를 위해 신경망의 가중치를 학습해 나가는 과정
2. 손실함수
   1. 신경망 학습을 위해서는 경사하강법을 사용
3. 시그모이드 함수의 문제점
   1. 기울기 소실의 문제
      1. 역전파를 거듭해서 진행하다보면 어느순간 0에 가까운 에러를 인식하게 된다.
4. 최적화함수
   1. 경사하강법
      1. 모든데이터를 이용해 업데이트
   2. 확률적 경사하강법
      1. 확률적으로 선택된 하나의 데이터를 이용해 업데이트
      2. 경사하강법에 비해 안정성은 떨어지지만 속도는 빠르다
5. 배치사이즈
   1. 일반적으로 pc메모리의 한계 및 속도 저하 때문에 대부분의 경우에는 한번에 epoch에 모든 데이터를 한꺼번에 집어넣기 힘듦
   2. 배치사이즈를 줄임
      1. 메모리소모가 적음
      2. 학습속도가 느임
      3. 학습속도가 느린 만큼 좀 더 정확하게 학습
   3. 배치사이즈를 늘림
      1. 메모리 소모가 큼
      2. 학습속도가 빠름
      3. 정확도가 낮음
   4. 디폴트 값은 32
6. 모멘텀
   1. 경사하강법에 관성의 법칙을 적용해 현재batch 뿐만 아니라 이전 batch의 데이터까지 업데이트에 반영
   2. 가중치를 수정하기 전 이전 방향을 참고하여 업데이트
7. 합성곱신경망(CNN)
   1. 데니터의 특징들을 추출하는 
   2. MLP의 단점을 개선하기 위해 등장
      1. MLP는 이미지의 위치에 민감하게 동작하며 위치에 종속적인 결과를 얻게 됨
      2.  MLP로 이미지 인식을 하려면 크기와 위치를 비슷하게 맞춰야 함
      3. 그래서 특징을 추출해서 비교하는 CNN이 대두되게 됨
   3. 구조
      1. 특징추출부 + 분류(MLP)로 나뉜다.
   4. 패딩
      1. 필터의 크기로 인해 가장자리 부분의 데이터가 부족해서 입력과 출력 이미지의 크기가 달라지게 되는데 이를 보완하기 위해서 입력데이터의 가장자리 부분에 0을 미리 채워 넣는 것을 패딩이라고 함
      2. 패딩을 사용하면 특정 층의 입력과 출력의 크기를 같게 맞춰줄 수 있음
      3. 즉, 층이 깊어지면서 이미지의 크기가 줄어드는 것을 방지
   5. 축소샘플링
      1. 합성곱을 수행한 결과 신호를 다음 계층으로 전달할 때, 모든 정보를 전달하지 않고 일부만 샘플링 하여 넘겨주는 작업을 축소 샘플링이라고 함